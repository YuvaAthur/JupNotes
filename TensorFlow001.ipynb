{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TensorFlow MNIST for ML Beginners"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### MINST is a simple computer vision data. We will train a model to look at image and predict what digits they are\n",
    "##### Ref: https://www.tensorflow.org/versions/r0.7/tutorials/mnist/beginners/index.html "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully downloaded train-images-idx3-ubyte.gz 9912422 bytes.\n",
      "Extracting MNIST_data/train-images-idx3-ubyte.gz\n",
      "Successfully downloaded train-labels-idx1-ubyte.gz 28881 bytes.\n",
      "Extracting MNIST_data/train-labels-idx1-ubyte.gz\n",
      "Successfully downloaded t10k-images-idx3-ubyte.gz 1648877 bytes.\n",
      "Extracting MNIST_data/t10k-images-idx3-ubyte.gz\n",
      "Successfully downloaded t10k-labels-idx1-ubyte.gz 4542 bytes.\n",
      "Extracting MNIST_data/t10k-labels-idx1-ubyte.gz\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "mnist = input_data.read_data_sets(\"MNIST_data/\",one_hot=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### The downloaded data is split into 3 parts : mnist.train with 55000 training data, mnist.test of 10000 points and mnist.validation of 5000 points.\n",
    "\n",
    "##### mnist.train.images and mnist.train.labels "
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "###### Approach: \n",
    " 1) Flatten image to 28x28 = 784 dim vector\n",
    " 2) mnist.train.images is a tensor (n-array) with shape [55000, 784]\n",
    " 3) Corresponding labels in mnist.train.labels are numbers between 0 and 9\n",
    " 3.1) Represent these as \"one-hot vectors\" or has 1 in only ONE dimension.\n",
    " 3.2) So, we need a 10 dim vector to represent each digit as a 1-hot vector\n",
    " 4) Our model should give max probability to the label of the image.\n",
    " 5) Softmax regression is a natural, simple model\n",
    " 5.1) softmax (x) = normalize(exp(x))\n",
    " 5.2) y = softmax(Wx + b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x = tf.placeholder(tf.float32,[None,784])\n",
    "W = tf.Variable(tf.zeros([784,10]))\n",
    "b = tf.Variable(tf.zeros([10]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y = tf.nn.softmax(tf.matmul(x,W)+b)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Note that we multiple x to W to take care of x as 2D tensor."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### That is it! Our model has now been defined :)!! \n",
    "TensorFlow is designed to express manu kind of numberical computations - from machine learning to physics simulations. And once defined, it can run on different devices - CPU, GPU, or even phones. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training \n",
    "To make our model good this is what we need to do. Typically this means optimizing some cost function. One nice cost function is \"cross-entropy\".\n",
    "\n",
    "To implement cross-entropy, set up a new placemholder to input the correct answers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y_ = tf.placeholder(tf.float32, [None, 10])\n",
    "cross_entropy = -tf.reduce_sum(y_*tf.log(y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that this is sum of cross-entropies for all the images we looked at. In this example, we have 100 images in each batch. Choosing 100 as sample size is better than 1 to get a fair indication of how good our model is.  "
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Approach:\n",
    "1. We want to reduce cross_entropy.\n",
    "2. We have set up the computation graph.\n",
    "3. Now let TensorFlow use backpropagation algorithm to determine the variables that affect the cost\n",
    "4. Then TensorFlow can apply optimization algorithm to modify the variables and reduce the cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train_step = tf.train.GradientDescentOptimizer(0.01).minimize(cross_entropy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What TensorFlow actually does here behind the scenes is it adds new operations to graph which implement backpropagation and gradient descent. Then it gives back a single operation which when run will do a step of descent training, slightly tweaking the variable to reduce the cost."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model is ready to be launched after initializing the variables created. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "init = tf.initialize_all_variables()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Launch a session and run the operation to initialize the variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sess = tf.Session()\n",
    "sess.run(init)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we are ready to train.\n",
    "\n",
    "Loop over each batch of 100 random data points, and run the train_step for each batch. Store the output in the placeholders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for i in range (1000) :\n",
    "    batch_xs, batch_ys = mnist.train.next_batch(100)\n",
    "    sess.run(train_step,feed_dict={x: batch_xs, y_:batch_ys})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Usin small batches of random data is called stochastic training - in this case stochastic gradient descent. Using all data for every training step is better but expensive. Smaller random samples is cheaper and offers much of the same benefit."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluating Model\n",
    "\n",
    "For each input, the label predicted has the highest probablity along y's 1-axis. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And the true label value is the highest value (recall 1-hot vector) of the 1-axis of y_."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By comparing the predicted max probability to the actual label, we can test the reliability of our model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "correct_prediction = tf.equal(tf.argmax(y,1),tf.argmax(y_,1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "argmax gives the index of the highest entry on that axis in the tensor. \n",
    "Now we can compute the accuracy of the model on the test data and print it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction,tf.float32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9191\n"
     ]
    }
   ],
   "source": [
    "print(sess.run(accuracy, feed_dict={x:mnist.test.images,y_:mnist.test.labels}))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The value should be around 91%.\n",
    "\n",
    "Is 91% good enough? Well, not really - in fact it is pretty bad. The best models can get to 99.7% accuracy! (see http://rodrigob.github.io/are_we_there_yet/build/classification_datasets_results.html - but not clear how to interpret the error % unit as there are some which have more than 1.00 %!!)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
